{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Analysis and Clean Up of Wiki Death Cause Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Spacy English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove specified phrases from a string\n",
    "def remove_phrases(text, phrases_to_remove):\n",
    "    for phrase in phrases_to_remove:\n",
    "        text = text.replace(phrase, '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process text with Spacy, remove stop words and unwanted phrases\n",
    "def spacy_process(text, phrases_to_remove):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    text = remove_phrases(text, phrases_to_remove)\n",
    "    doc = nlp(text)\n",
    "    return [token.text.lower() for token in doc if not token.is_stop and not token.is_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_cause_spacy(cause, key_terms, term_mappings):\n",
    "    if not isinstance(cause, str):  # Check if cause is a string\n",
    "        return \"unknown\"  # Return \"unknown\" for non-string inputs (like NaN)\n",
    "\n",
    "    # First check specific terms in the mappings\n",
    "    for term, category in term_mappings.items():\n",
    "        if term in cause:\n",
    "            return category\n",
    "\n",
    "    # Then check general terms\n",
    "    for term in key_terms:\n",
    "        if term in cause:\n",
    "            return term\n",
    "\n",
    "    return \"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_categorize_data_spacy(file_path):\n",
    "    # Load the CSV file\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Standardize No Information Entries and Normalize Text\n",
    "    no_info_terms = [\"not mentioned\", \"no relevant section found\", \"unknown\"]\n",
    "    data['Cause_of_Death'] = data['Cause_of_Death'].str.lower().apply(\n",
    "        lambda x: 'unknown' if isinstance(x, str) and any(term in x for term in no_info_terms) else x)\n",
    "\n",
    "    # List of phrases to remove\n",
    "    phrases_to_remove = [\"cause of death: \", \"death cause: \", \"complications from \", \"the cause of death was \"]\n",
    "\n",
    "    # Process text and remove unwanted phrases\n",
    "    data['Cause_of_Death'] = data['Cause_of_Death'].apply(lambda x: ' '.join(spacy_process(x, phrases_to_remove)) if isinstance(x, str) else x)\n",
    "\n",
    "    # Key terms and their mappings\n",
    "    key_terms = ['cancer', 'heart', 'stroke', 'accident', 'suicide', 'murder',\n",
    "                 'organ failure', 'pneumonia', 'respiratory', 'natural causes',\n",
    "                 'tumor', 'diabetes', 'brain', 'poisoning', \n",
    "                 'liver', 'illness', 'als', 'kidney', 'assassination', 'tuberculosis', \n",
    "                 'overdose', 'alzheimer', 'parkinson', 'drowning', 'covid-19', 'aids', \n",
    "                 'bronchitis', 'surgery', 'fever', 'infection', 'blood', 'hemorrhage', 'asphyxiation']\n",
    "    \n",
    "    term_mappings = {'injury': 'accident', 'cardiac arrest': 'heart', 'blunt trauma': 'accident', 'leukemia': 'cancer',\n",
    "                     'cardiac':'heart', 'myeloma': 'cancer', 'cerebral': 'brain', 'gunshot': 'murder', 'hanged':'suicide',\n",
    "                     'lymphoma': 'cancer', 'shot':'murder', 'mesothelioma':'cancer', 'stab': 'murder',\n",
    "                     'cirrhosis':'liver', 'crash':'accident', 'collision':'accident', 'dementia':'alzheimer',\n",
    "                     'fall':'accident', 'hanging':'suicide', 'cardiovascular':'heart', 'knife wound':'murder',\n",
    "                     'unspecified': 'unknown', 'emphysema':'respiratory', 'aortic':'heart', 'thrombosis':'blood',\n",
    "                     'coronary':'heart', 'lung':'respiratory', 'pulmonary': 'respiratory'}\n",
    "\n",
    "    # Categorize the causes of death\n",
    "    data['Categorized_Cause'] = data['Cause_of_Death'].apply(lambda x: categorize_cause_spacy(x, key_terms, term_mappings))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = '../nndb_wiki_death_lookup/wiki_died_output.csv'\n",
    "cleaned_data_spacy = clean_and_categorize_data_spacy(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Presist this analysis to a CSV\n",
    "cleaned_data_spacy.to_csv('seed_nndb_death_causes.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Cause_of_Death Categorized_Cause\n",
      "2237                                    inhalation gases             other\n",
      "13842                                      renal failure             other\n",
      "2300   insufficient information provided determine ca...             other\n",
      "14526                          cause death provided text             other\n",
      "15988                    epileptic seizure complications             other\n",
      "1423                                           specified             other\n",
      "15767  hemorrhoidal condition leading bladder retenti...             other\n",
      "2523                                    synovial sarcoma             other\n",
      "7363   streptococcal toxic shock syndrome caused stre...             other\n",
      "15199                 cause edwin land death undisclosed             other\n",
      "11569                              myocardial infarction             other\n",
      "7257   statement context provide information cause death             other\n",
      "13584             cause death provided given information             other\n",
      "8151                     cause death provided given text             other\n",
      "7535   acute toxic nephritis secondary acute chronic ...             other\n",
      "3815                                              uremia             other\n",
      "15391  cause death henry demarest lloyd specified giv...             other\n",
      "11471                                   renal inadequacy             other\n",
      "9151                                adams died influenza             other\n",
      "9280                                cause death provided             other\n",
      "14192       aplastic anemia long term radiation exposure             other\n",
      "12458                                   arteriosclerosis             other\n",
      "6167                cte chronic traumatic encephalopathy             other\n",
      "10584                                          nephritis             other\n",
      "15201                                       burned stake             other\n"
     ]
    }
   ],
   "source": [
    "filtered_unknown = cleaned_data_spacy[cleaned_data_spacy['Cause_of_Death'] != 'unknown']\n",
    "filtered_data = filtered_unknown[filtered_unknown['Categorized_Cause'] == 'other']\n",
    "print(filtered_data[['Cause_of_Death', 'Categorized_Cause']].sample(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Categorized_Cause\n",
       "other             10986\n",
       "heart              1303\n",
       "cancer             1055\n",
       "respiratory         437\n",
       "natural causes      319\n",
       "stroke              312\n",
       "pneumonia           285\n",
       "accident            254\n",
       "murder              212\n",
       "suicide             165\n",
       "brain               101\n",
       "alzheimer            77\n",
       "illness              75\n",
       "kidney               72\n",
       "liver                57\n",
       "overdose             50\n",
       "tuberculosis         48\n",
       "assassination        44\n",
       "unknown              41\n",
       "parkinson            40\n",
       "diabetes             37\n",
       "tumor                36\n",
       "blood                35\n",
       "surgery              34\n",
       "fever                32\n",
       "poisoning            30\n",
       "aids                 28\n",
       "infection            26\n",
       "als                  16\n",
       "hemorrhage           16\n",
       "drowning             14\n",
       "asphyxiation         12\n",
       "bronchitis           12\n",
       "organ failure        10\n",
       "covid-19             10\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data_spacy['Categorized_Cause'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Remaing Word Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_common_words(common_words):\n",
    "    for word, freq in common_words:\n",
    "        print(f\"Word: '{word}', Frequency: {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: 'death', Frequency: 293\n",
      "Word: 'cause', Frequency: 268\n",
      "Word: 'provided', Frequency: 195\n",
      "Word: 'information', Frequency: 164\n",
      "Word: 'given', Frequency: 121\n",
      "Word: 'specified', Frequency: 41\n",
      "Word: 'disease', Frequency: 30\n",
      "Word: 'text', Frequency: 28\n",
      "Word: 'acute', Frequency: 24\n",
      "Word: 'complications', Frequency: 23\n",
      "Word: 'undisclosed', Frequency: 22\n",
      "Word: 'died', Frequency: 21\n",
      "Word: 'mentioned', Frequency: 18\n",
      "Word: 'age', Frequency: 15\n",
      "Word: 'causes', Frequency: 15\n",
      "Word: 'mention', Frequency: 14\n",
      "Word: 'multiple', Frequency: 11\n",
      "Word: 'health', Frequency: 11\n",
      "Word: 'attack', Frequency: 11\n",
      "Word: 'intoxication', Frequency: 11\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def most_common_words(dataframe, column_name):\n",
    "    text = ' '.join(dataframe[column_name].dropna())  # Join all text and handle NaN values\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Filter tokens that are stop words or punctuations\n",
    "    words = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct and not token.is_space]\n",
    "\n",
    "    # Count the words\n",
    "    word_freq = Counter(words)\n",
    "\n",
    "    return word_freq.most_common(20)  # You can adjust the number to get more or less frequent words\n",
    "\n",
    "# Usage\n",
    "common_words = most_common_words(filtered_data, 'Cause_of_Death')\n",
    "print_common_words(common_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Bi-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x158eef190>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Spacy English language model\n",
    "nlp = English()\n",
    "nlp.add_pipe('sentencizer')  # Add sentencizer to the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_bigrams(dataframe, column_name):\n",
    "    text = ' '.join(dataframe[column_name].dropna())  # Join all text and handle NaN values\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Create bigrams\n",
    "    bigrams = []\n",
    "    for sent in doc.sents:\n",
    "        tokens = [token.text.lower() for token in sent if not token.is_stop and not token.is_punct and not token.is_space]\n",
    "        bigrams.extend(zip(tokens, tokens[1:]))\n",
    "\n",
    "    # Count the bigrams\n",
    "    bigram_freq = Counter(bigrams)\n",
    "\n",
    "    return bigram_freq.most_common(30)  # Adjust number for more/less frequent bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: '('cause', 'death')', Frequency: 249\n",
      "Word: '('death', 'provided')', Frequency: 117\n",
      "Word: '('given', 'information')', Frequency: 93\n",
      "Word: '('provided', 'given')', Frequency: 83\n",
      "Word: '('information', 'provided')', Frequency: 39\n",
      "Word: '('provided', 'cause')', Frequency: 27\n",
      "Word: '('provided', 'information')', Frequency: 24\n",
      "Word: '('information', 'cause')', Frequency: 23\n",
      "Word: '('death', 'specified')', Frequency: 21\n",
      "Word: '('given', 'text')', Frequency: 15\n",
      "Word: '('undisclosed', 'causes')', Frequency: 14\n",
      "Word: '('mention', 'cause')', Frequency: 13\n",
      "Word: '('death', 'mentioned')', Frequency: 12\n",
      "Word: '('specified', 'given')', Frequency: 12\n",
      "Word: '('specified', 'provided')', Frequency: 11\n",
      "Word: '('provided', 'text')', Frequency: 9\n",
      "Word: '('provided', 'mention')', Frequency: 8\n",
      "Word: '('death', 'cause')', Frequency: 8\n",
      "Word: '('information', 'given')', Frequency: 7\n",
      "Word: '('blunt', 'force')', Frequency: 7\n",
      "Word: '('death', 'given')', Frequency: 6\n",
      "Word: '('text', 'cause')', Frequency: 6\n",
      "Word: '('myocardial', 'infarction')', Frequency: 6\n",
      "Word: '('declining', 'health')', Frequency: 6\n",
      "Word: '('force', 'trauma')', Frequency: 6\n",
      "Word: '('renal', 'failure')', Frequency: 6\n",
      "Word: '('determine', 'cause')', Frequency: 5\n",
      "Word: '('explicitly', 'mentioned')', Frequency: 5\n",
      "Word: '('death', 'information')', Frequency: 5\n",
      "Word: '('old', 'age')', Frequency: 5\n"
     ]
    }
   ],
   "source": [
    "common_bigrams = most_common_bigrams(filtered_data, 'Cause_of_Death')\n",
    "print_common_words(common_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
