{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConfusionMatrixDisplay\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metrics \n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import ast\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn import metrics \n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"nndb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"IS_DECEASED\"] = df[\"DIED\"].notnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.RISK_FACTORS.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse and clean individual strings\n",
    "def parse_and_clean(s):\n",
    "    # Check for NaN (float) values\n",
    "    if isinstance(s, float):\n",
    "        return []\n",
    "    # Safely evaluate the string as a list\n",
    "    parsed_list = ast.literal_eval(s)\n",
    "    # Clean each element in the list\n",
    "    return [item.strip() for item in parsed_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = [parse_and_clean(row) for row in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data: strip spaces and convert to lowercase\n",
    "normalized_data = [[factor.strip().lower() for factor in sublist] for sublist in cleaned_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({'RISK_FACTORS': normalized_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the list and get unique risk factors, normalized\n",
    "unique_risk_factors = set(factor for sublist in normalized_data for factor in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrames for each risk factor and store them in a list\n",
    "encoded_frames = []\n",
    "for risk_factor in unique_risk_factors:\n",
    "    # Create a column for each risk factor with 0s\n",
    "    risk_factor_col = pd.Series([0] * len(df2))\n",
    "    \n",
    "    # Update the column with 1 where the risk factor is present\n",
    "    for i, row in enumerate(df2['RISK_FACTORS']):\n",
    "        if isinstance(row, list) and risk_factor in [item.lower().strip() for item in row]:\n",
    "            risk_factor_col.at[i] = 1\n",
    "\n",
    "    # Add the column to the list of DataFrames\n",
    "    temp_df = pd.DataFrame({risk_factor: risk_factor_col})\n",
    "    encoded_frames.append(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all the encoded DataFrames\n",
    "encoded_df = pd.concat(encoded_frames, axis=1)\n",
    "\n",
    "# Join the encoded DataFrame with the original DataFrame\n",
    "df = df.join(encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.obesity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original RISK_FACTORS column if necessary\n",
    "df.drop('RISK_FACTORS', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['GENDER'] = df['GENDER'].astype(\"category\")\n",
    "df['RACE'] = df['RACE'].astype(\"category\")\n",
    "df['OCCUPATION'] = df['OCCUPATION'].astype(\"category\")\n",
    "df['NATIONALITY'] = df['NATIONALITY'].astype(\"category\")\n",
    "df['BIRTHPLACE'] = df['BIRTHPLACE'].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'date_column' to datetime, coercing out-of-bounds dates to NaT\n",
    "df['BORN'] = pd.to_datetime(df['BORN'], errors='coerce')\n",
    "df['DIED'] = pd.to_datetime(df['DIED'], errors='coerce')\n",
    "\n",
    "# Filter to keep only the rows where the date is >= 1700-01-01\n",
    "df = df[df['BORN'] >= pd.Timestamp('1700-01-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y = df['IS_DECEASED']\n",
    "df_X = df.drop(columns=['IS_DECEASED', 'DIED', 'LOCATION_OF_DEATH', 'CAUSE_OF_DEATH', 'NAME', 'AKA', 'LINK', 'BORN', 'EXECUTIVE_SUMMARY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_column = df['NAME']\n",
    "age_column = df['AGE']\n",
    "is_dead_column = df['IS_DECEASED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into 30% test and 70% training\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count examples in each class\n",
    "counter = Counter(df_y)\n",
    "# estimate scale_pos_weight value\n",
    "estimate = counter[0] / counter[1]\n",
    "print('Estimate: %.3f' % estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipe(clf):\n",
    "\n",
    "    pipeline = Pipeline([('clf', clf)])\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier(random_state=42, \n",
    "                        verbosity=0, \n",
    "                        tree_method=\"hist\",\n",
    "                        enable_categorical=True)\n",
    "\n",
    "pipeline = create_pipe(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_list = []\n",
    "xgb_cols = pipeline['clf'].get_booster().feature_names\n",
    "feat_imp = pipeline['clf'].feature_importances_\n",
    "\n",
    "total_importance = 0\n",
    "# Print the name and gini importance of each feature\n",
    "for feature in zip(xgb_cols, feat_imp):\n",
    "    feat_list.append(feature)\n",
    "    total_importance += feature[1]\n",
    "        \n",
    "# create DataFrame using data\n",
    "df_imp = pd.DataFrame(feat_list, columns =['FEATURE', 'IMPORTANCE']).sort_values(by='IMPORTANCE', ascending=False)\n",
    "df_imp['SUMMED_TOTAL'] = df_imp['IMPORTANCE'].cumsum()\n",
    "df_imp.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion(pipeline):\n",
    "    ''' take a supplied pipeline and run it against the train-test spit \n",
    "    and product scoring results.'''\n",
    "    \n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    print(metrics.classification_report(y_test, y_pred, digits=3))\n",
    "        \n",
    "    ConfusionMatrixDisplay.from_predictions(y_test, \n",
    "                                            y_pred, \n",
    "                                            cmap=plt.cm.Blues)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('02_confusion_matrix.png', dpi=300);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_confusion(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_list = []\n",
    "xgb_cols = pipeline['clf'].get_booster().feature_names\n",
    "feat_imp = pipeline['clf'].feature_importances_\n",
    "\n",
    "total_importance = 0\n",
    "# Print the name and gini importance of each feature\n",
    "for feature in zip(xgb_cols, feat_imp):\n",
    "    feat_list.append(feature)\n",
    "    total_importance += feature[1]\n",
    "        \n",
    "# create DataFrame using data\n",
    "df_imp = pd.DataFrame(feat_list, columns =['FEATURE', 'IMPORTANCE']).sort_values(by='IMPORTANCE', ascending=False)\n",
    "df_imp['SUMMED_TOTAL'] = df_imp['IMPORTANCE'].cumsum()\n",
    "df_imp.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict the outcome variable based on the model\n",
    "probs = pipeline.predict_proba(df_X)\n",
    "\n",
    "# Get the Win probability for the `win` class\n",
    "probs = probs[:,1]\n",
    "probs = probs.round(3)\n",
    "\n",
    "# Combine predictions with the name column\n",
    "result_df = pd.DataFrame({\n",
    "    'NAME': name_column,\n",
    "    'AGE': age_column,\n",
    "    'IS_DECEASED': is_dead_column,\n",
    "    'PREDICTION': probs,\n",
    "})\n",
    "\n",
    "# Add the probability percentage to the DataFrame\n",
    "# X['last_prediction_date'] = pd.Timestamp.today().strftime('%Y-%m-%d')\n",
    "# X['convert_probability'] = probs.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = result_df.sort_values('PREDICTION', ascending=False)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = result_df[result_df['IS_DECEASED'] == False]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_leads = filtered_df[filtered_df['PREDICTION'] >= 0.75]\n",
    "df_leads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_leads.to_csv('dead_pool_leads.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
